<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Real-Time 3D Vision-Language Embedding Mapping</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Real-Time 3D Vision-Language Embedding Mapping</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Anonymous</a></span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Submitted to RAL</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
       This work introduces a method for real-time 3D mapping of image embeddings using only RGB-D streams, without requiring ground-truth camera poses or environment-specific pretraining. Our approach is environment- and task-agnostic, providing a foundation for language-conditioned robotic tasks.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          A metric-accurate semantic 3D representation is essential for many robotic tasks. This work proposes a simple, yet powerful, way to integrate the 2D embeddings of a Vision-Language Model in a metric-accurate 3D representation at real-time. We combine a local embedding masking strategy, for a more distinct embedding distribution, with a confidence-weighted 3D integration for more reliable 3D embeddings.
          The resulting metric-accurate embedding representation is task-agnostic and can represent semantic concepts on a global multi-room- as well as on a local object-level. This enables a variety of interactive robotic applications that require the localisation of objects-of-interest via natural language. We evaluate our approach on a variety of real-world sequences and demonstrate that these strategies achieve a more accurate object-of-interest localisation while improving the runtime performance in order to meet our real-time constraints. We further demonstrate the versatility of our approach in a variety of interactive handheld, mobile robotics and manipulation tasks, requiring only raw image data.
          </p>
        </div>
      </div>
    </div>
  </div>

  <div style="display: flex; justify-content: center; width: 100%; margin: 20px 0;">
  <video
    poster="banner"
    id="tree"
    autoplay
    controls
    loop
    style="width: 45%; height: auto; border-radius: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.2);"
  >
    <source src="static/videos/banner_video.mp4" type="video/mp4">
  </video>
</div>
</section>
<!-- End paper abstract -->

<br><br>
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Approach</h2>
        <div class="content has-text-justified">
          <p>
          We aim to construct a metric-accurate 3D map using only a stream of RGB-D images. The representation encodes geometry and semantics, integrating embeddings and camera poses over time, in a fully online and memory-limited fashion. The map is incrementally updated, queried in real time, and supports open-ended language interaction.
          </p>
          <div style="display: flex; justify-content: center; overflow: visible;">
          <figure class="image mt-3">
            <img src="static/images/overview.png" alt="Approach diagram" style="width: 100%;max-width: 1800px; height: auto;">
          </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Row 1: Camera Localisation & Vision-Language Embeddings -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-variable is-8">
      
      <!-- Camera Localisation -->
      <div class="column is-half">
        <h3 class="title is-4">Camera Localisation</h3>
        <p>
        We initialise the system by estimating camera pose through RGB-D odometry using a coarse-to-fine strategy. To ensure robustness, we run a separate dense tracking module alongside the main semantic map. This module maintains a signed distance field using a voxel hash map and enables stable pose estimation through frame-to-model alignment. By decoupling tracking from embedding integration, we preserve both high localisation accuracy and real-time efficiency. This separation is critical to avoid cumulative degradation in both geometry and semantic quality.
        </p>
        <figure class="image is-4by3 mt-3">
          <img src="static/images/placeholder_localisation.png" alt="Camera Localisation">
        </figure>
      </div>

      <!-- Segment-Aligned Vision-Language Embeddings -->
      <div class="column is-half">
        <h3 class="title is-4">Segment-Aligned Vision-Language Embeddings</h3>
        <p>
        To encode rich semantic cues, we extract pixel-aligned vision-language embeddings for each frame. We use the Segment Anything model (SAM) to generate object-aware masks, which are then passed through a CLIP encoder to obtain segment-level features. Unlike prior approaches that rely on bounding box crops, our method directly associates segment embeddings with pixel coordinates. This alignment ensures precise spatial grounding of semantic features while avoiding dilution from irrelevant context.
        </p>
        <figure class="image is-4by3 mt-3">
          <img src="static/images/placeholder_embeddings.png" alt="2D Embeddings">
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Row 2: Masking & 3D Integration -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-variable is-8">
      
      <!-- Refinement and Masking -->
      <div class="column is-half">
        <h3 class="title is-4">Refinement and Masking Strategy</h3>
        <p>
        We improve segment coverage by sampling unsegmented regions and querying them again through SAM, filling in missed areas without relying on heavier models. To avoid embedding overlap across nearby objects, we apply a masking strategy where only foreground pixels are retained, while background regions are replaced with a fixed value. This helps ensure the resulting embeddings are more distinctive, leading to sharper and more separable responses during language queries. We also discard global image embeddings, as they tend to blur local semantic distinctions.
        </p>
        <figure class="image is-4by3 mt-3">
          <img src="static/images/placeholder_masking.png" alt="Masking and Refinement">
        </figure>
      </div>

      <!-- 3D Embedding Integration -->
      <div class="column is-half">
        <h3 class="title is-4">3D Embedding Integration</h3>
        <p>
        Segment-level embeddings are lifted into 3D using the estimated pose and depth image. To ensure efficient storage and robustness, embeddings are integrated into the global map using a confidence-weighted fusion that prioritises well-observed points. The 3D map is maintained under strict memory constraints by adaptive downsampling while preserving semantic coverage. Embeddings are incrementally updated as new data arrives, allowing the system to represent spatial and semantic structure densely and in real time.
        </p>
        <figure class="image is-4by3 mt-3">
          <img src="static/images/placeholder_integration.png" alt="3D Integration">
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Final Full-Width Section: Querying -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h3 class="title is-4">Text-Based 3D Querying</h3>
        <figure class="image is-4by3 mt-3">
          <img src="static/images/placeholder_querying.png" alt="Text-Based 3D Querying">
        </figure>
        <p>
        At inference time, users can query the map using arbitrary text. A CLIP text embedding is compared with the 3D semantic map to produce a similarity heatmap. We run multiple queries simultaneously, including generic prompts like "object", in order to suppress noisy matches and boost relevant responses. The output is refined through clustering in both feature and geometric space, yielding compact and accurate 3D segments corresponding to the target query. This allows intuitive, language-driven interaction with real-world environments.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <br><br>
        <h2 class="title is-3">Evaluation</h2>
        <div class="content has-text-justified">
          <p>
          We aim to construct a metric-accurate 3D map using only a stream of RGB-D images. The representation encodes geometry and semantics, integrating embeddings and camera poses over time, in a fully online and memory-limited fashion. The map is incrementally updated, queried in real time, and supports open-ended language interaction.
          </p>
          <div style="display: flex; justify-content: center; overflow: visible;">
          <figure class="image mt-3">
            <img src="static/images/overview.png" alt="Approach diagram" style="width: 100%;max-width: 1800px; height: auto;">
          </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->



<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
